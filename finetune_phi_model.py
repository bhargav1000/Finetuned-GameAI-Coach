#!/usr/bin/env python3
"""
Fine-tune Microsoft Phi-3.5-mini-instruct for Fighting Game AI Assistant

This script fine-tunes the Phi-3.5-mini-instruct model using the collected
training data to create a specialized fighting game strategy assistant.

Requirements:
- CUDA-capable GPU (8GB+ VRAM recommended)
- Python 3.8+
- Training data generated by generate_training_data.py

Usage:
    python finetune_phi_model.py

The script will:
1. Download Phi-3.5-mini-instruct to model/ directory
2. Load training data from training_data/summary/training_dataset.jsonl
3. Fine-tune using LoRA (Low-Rank Adaptation)
4. Save the fine-tuned model for inference
5. Provide evaluation metrics and sample outputs
"""

import json
import os
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import logging
from tqdm.auto import tqdm

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Check system capabilities
CUDA_AVAILABLE = torch.cuda.is_available()
MPS_AVAILABLE = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
DEVICE = "cuda" if CUDA_AVAILABLE else "mps" if MPS_AVAILABLE else "cpu"

logger.info(f"üñ•Ô∏è  System Info: CUDA={CUDA_AVAILABLE}, MPS={MPS_AVAILABLE}, Device={DEVICE}")

# Check transformers version compatibility
try:
    import transformers
    transformers_version = transformers.__version__
    logger.info(f"üì¶ Transformers version: {transformers_version}")
    
    # Check for known compatibility issues
    from packaging import version
    if version.parse(transformers_version) >= version.parse("4.45.0"):
        logger.warning("‚ö†Ô∏è  Warning: Transformers version may have compatibility issues with Phi-3.5")
        logger.warning("‚ö†Ô∏è  Consider downgrading: pip install transformers==4.44.2")
except ImportError:
    logger.warning("‚ö†Ô∏è  Could not check transformers version")

# Check if unsloth is available, if not provide installation instructions
try:
    from unsloth import FastLanguageModel
    from unsloth import is_bfloat16_supported
    UNSLOTH_AVAILABLE = True
except ImportError:
    UNSLOTH_AVAILABLE = False
    print("‚ö†Ô∏è  Unsloth not found. Install with:")
    print("pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'")

# Alternative: Use transformers + peft if unsloth not available
try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM, 
        TrainingArguments, 
        Trainer,
        DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, TaskType, get_peft_model
    from datasets import Dataset
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è  Transformers not found. Install with:")
    print("pip install transformers peft datasets accelerate bitsandbytes trl")

class PhiGameAssistantTrainer:
    """Fine-tune Phi-3.5-mini-instruct for fighting game strategy assistance"""
    
    def __init__(self, 
                 model_name: str = "microsoft/Phi-3.5-mini-instruct",
                 training_data_path: str = "training_data/summary/training_dataset.jsonl",
                 model_dir: str = "model",
                 output_dir: str = "model/fine_tuned",
                 max_seq_length: int = 2048):
        
        self.model_name = model_name
        self.training_data_path = Path(training_data_path)
        self.model_dir = Path(model_dir)
        self.output_dir = Path(output_dir)
        self.max_seq_length = max_seq_length
        
        # Create model directory structure
        self.model_dir.mkdir(exist_ok=True)
        self.base_model_dir = self.model_dir / "cache"  # For HuggingFace cache
        self.base_model_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Training hyperparameters (optimized for your 11,741 examples)
        # Adjust based on available hardware
        if DEVICE == "cpu":
            batch_size = 1
            optim = "adamw_torch"
        elif DEVICE == "mps":
            batch_size = 2  # MPS can handle small batches well
            optim = "adamw_torch"  # MPS doesn't support 8bit optimizers
        else:  # CUDA
            batch_size = 2
            optim = "adamw_8bit"
        
        self.training_config = {
            "per_device_train_batch_size": batch_size,
            "gradient_accumulation_steps": 8 if DEVICE == "cpu" else 4,
            "warmup_steps": 10,
            "max_steps": 50 if DEVICE == "cpu" else 200,  # Reduced for CPU
            "learning_rate": 2e-4,
            "fp16": CUDA_AVAILABLE,  # FP16 mixed precision only works on CUDA in transformers
            "bf16": False,  # Disable for compatibility
            "logging_steps": 5,
            "optim": optim,
            "weight_decay": 0.01,
            "lr_scheduler_type": "linear",
            "seed": 3407,
            "save_strategy": "steps",
            "save_steps": 25 if DEVICE == "cpu" else 50,
            "eval_steps": 25 if DEVICE == "cpu" else 50,
            "save_total_limit": 3,
        }
        
        # LoRA configuration (optimized for Phi-3.5)
        self.lora_config = {
            "r": 16,
            "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                             "gate_proj", "up_proj", "down_proj"],
            "lora_alpha": 16,
            "lora_dropout": 0,
            "bias": "none",
            "use_gradient_checkpointing": "unsloth" if UNSLOTH_AVAILABLE else True,
            "random_state": 3407,
            "use_rslora": False,
            "loftq_config": None,
        }
        
        self.model = None
        self.tokenizer = None
        self.dataset = None



    def load_training_data(self) -> List[Dict]:
        """Load and validate training data"""
        logger.info(f"üìö Loading training data from {self.training_data_path}")
        
        if not self.training_data_path.exists():
            raise FileNotFoundError(f"Training data not found: {self.training_data_path}")
        
        # First pass to count total lines for progress bar
        with open(self.training_data_path, 'r') as f:
            total_lines = sum(1 for _ in f)
        
        training_examples = []
        with open(self.training_data_path, 'r') as f:
            with tqdm(total=total_lines, desc="üìñ Loading training data", unit="examples") as pbar:
                for line_num, line in enumerate(f, 1):
                    try:
                        example = json.loads(line.strip())
                        
                        # Validate required fields
                        required_fields = ['instruction', 'input', 'output']
                        if all(field in example for field in required_fields):
                            training_examples.append(example)
                        else:
                            logger.warning(f"Skipping invalid example at line {line_num}: missing required fields")
                            
                    except json.JSONDecodeError:
                        logger.warning(f"Skipping invalid JSON at line {line_num}")
                    
                    pbar.update(1)
                    if line_num % 1000 == 0:  # Update description every 1000 lines
                        pbar.set_postfix(valid=len(training_examples), invalid=line_num-len(training_examples))
        
        logger.info(f"‚úÖ Loaded {len(training_examples)} valid training examples")
        return training_examples

    def format_training_prompt(self, example: Dict) -> str:
        """Format training example into Phi-3.5 chat template"""
        
        # Phi-3.5 chat template format
        prompt = f"""<|user|>
{example['instruction']}

Game State: {example['input']}<|end|>
<|assistant|>
{example['output']}<|end|>"""
        
        return prompt

    def prepare_dataset(self, training_examples: List[Dict]) -> Dataset:
        """Prepare dataset for training"""
        logger.info("üîÑ Preparing dataset...")
        
        # Format prompts with progress bar
        formatted_texts = []
        with tqdm(training_examples, desc="üî§ Formatting prompts", unit="examples") as pbar:
            for example in pbar:
                formatted_text = self.format_training_prompt(example)
                formatted_texts.append(formatted_text)
        
        # Create HuggingFace dataset
        logger.info("üì¶ Creating HuggingFace dataset...")
        dataset = Dataset.from_dict({"text": formatted_texts})
        
        # Tokenize with progress bar
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=self.max_seq_length,
                return_overflowing_tokens=False,
            )
        
        logger.info("üî¢ Tokenizing dataset...")
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True,
            remove_columns=dataset.column_names,
            desc="Tokenizing"
        )
        logger.info(f"‚úÖ Dataset prepared with {len(tokenized_dataset)} examples")
        
        return tokenized_dataset

    def load_model_unsloth(self):
        """Load model using Unsloth (recommended for efficiency)"""
        logger.info("üöÄ Loading model with Unsloth...")
        
        # Always use the HuggingFace model name for Unsloth
        # Unsloth will handle caching automatically
        model_path = self.model_name
        
        # Configure quantization based on available hardware
        load_in_4bit = CUDA_AVAILABLE  # Only use 4-bit on CUDA
        
        with tqdm(total=3, desc="üì• Loading model components") as pbar:
            pbar.set_description("üì• Downloading/loading model")
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path,
                max_seq_length=self.max_seq_length,
                dtype=None,  # Auto-detect
                load_in_4bit=load_in_4bit,
                trust_remote_code=True,
                device_map="auto" if CUDA_AVAILABLE else None,
            )
            pbar.update(1)
            
            pbar.set_description("üîß Adding LoRA adapters")
            # Add LoRA adapters
            model = FastLanguageModel.get_peft_model(
                model,
                **self.lora_config
            )
            pbar.update(1)
            
            pbar.set_description("‚úÖ Model ready")
            pbar.update(1)
        
        self.model = model
        self.tokenizer = tokenizer
        
        logger.info("‚úÖ Model loaded successfully with Unsloth")

    def load_model_transformers(self):
        """Load model using standard transformers (fallback)"""
        logger.info("üîÑ Loading model with Transformers...")
        
        # Always use the HuggingFace model name for transformers
        # The local cache will be handled automatically by transformers
        model_path = self.model_name
        
        with tqdm(total=4, desc="üì• Loading model components") as pbar:
            pbar.set_description("üî§ Loading tokenizer")
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                cache_dir=str(self.base_model_dir)
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            pbar.update(1)
        
            pbar.set_description("üß† Configuring model settings")
            # Load model with appropriate settings for the device
            model_kwargs = {
                "trust_remote_code": True,
                "cache_dir": str(self.base_model_dir)
            }
            
            if CUDA_AVAILABLE:
                # CUDA settings
                model_kwargs.update({
                    "torch_dtype": torch.float16,
                    "device_map": "auto",
                    "load_in_4bit": True,  # Requires bitsandbytes
                })
            elif MPS_AVAILABLE:
                # Apple Silicon settings - use float32 for better compatibility
                model_kwargs.update({
                    "torch_dtype": torch.float32,  # Use float32 for MPS compatibility
                    "device_map": None,  # MPS doesn't support device_map
                    "low_cpu_mem_usage": True,  # Optimize memory usage on Mac
                })
            else:
                # CPU settings
                model_kwargs.update({
                    "torch_dtype": torch.float32,  # Use float32 on CPU
                    "device_map": None,
                })
            pbar.update(1)
            
            pbar.set_description("üì• Loading model weights")
            model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
            pbar.update(1)
            
            pbar.set_description("üéØ Moving to device")
            # Move to appropriate device if not using device_map
            if not CUDA_AVAILABLE:
                model = model.to(DEVICE)
            pbar.update(1)
        
        # Add LoRA
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.lora_config["r"],
            lora_alpha=self.lora_config["lora_alpha"],
            lora_dropout=self.lora_config["lora_dropout"],
            target_modules=self.lora_config["target_modules"],
        )
        
        self.model = get_peft_model(model, peft_config)
        logger.info("‚úÖ Model loaded successfully with Transformers")

    def train_unsloth(self, dataset: Dataset):
        """Train using Unsloth trainer"""
        logger.info("üèãÔ∏è Starting training with Unsloth...")
        
        from trl import SFTTrainer
        
        trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=dataset,
            dataset_text_field="text",
            max_seq_length=self.max_seq_length,
            dataset_num_proc=2,
            packing=False,  # Can make training 5x faster for short sequences
            args=TrainingArguments(
                per_device_train_batch_size=self.training_config["per_device_train_batch_size"],
                gradient_accumulation_steps=self.training_config["gradient_accumulation_steps"],
                warmup_steps=self.training_config["warmup_steps"],
                max_steps=self.training_config["max_steps"],
                learning_rate=self.training_config["learning_rate"],
                fp16=self.training_config["fp16"],
                bf16=self.training_config["bf16"],
                logging_steps=self.training_config["logging_steps"],
                optim=self.training_config["optim"],
                weight_decay=self.training_config["weight_decay"],
                lr_scheduler_type=self.training_config["lr_scheduler_type"],
                seed=self.training_config["seed"],
                output_dir=str(self.output_dir),
                save_strategy=self.training_config["save_strategy"],
                save_steps=self.training_config["save_steps"],
                save_total_limit=self.training_config["save_total_limit"],
                eval_strategy="no",  # No validation set for now
                report_to="none",  # Disable wandb/tensorboard
            ),
        )
        
        # Train the model
        trainer_stats = trainer.train()
        
        logger.info("üéâ Training completed!")
        logger.info(f"Training stats: {trainer_stats}")
        
        return trainer

    def train_transformers(self, dataset: Dataset):
        """Train using standard transformers trainer"""
        logger.info("üèãÔ∏è Starting training with Transformers...")
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            per_device_train_batch_size=self.training_config["per_device_train_batch_size"],
            gradient_accumulation_steps=self.training_config["gradient_accumulation_steps"],
            warmup_steps=self.training_config["warmup_steps"],
            max_steps=self.training_config["max_steps"],
            learning_rate=self.training_config["learning_rate"],
            fp16=self.training_config["fp16"],
            logging_steps=self.training_config["logging_steps"],
            optim=self.training_config["optim"],
            weight_decay=self.training_config["weight_decay"],
            lr_scheduler_type=self.training_config["lr_scheduler_type"],
            seed=self.training_config["seed"],
            save_strategy=self.training_config["save_strategy"],
            save_steps=self.training_config["save_steps"],
            save_total_limit=self.training_config["save_total_limit"],
            eval_strategy="no",
            report_to="none",
        )
        
        # Create trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
        )
        
        # Train
        trainer_stats = trainer.train()
        
        logger.info("üéâ Training completed!")
        logger.info(f"Training stats: {trainer_stats}")
        
        return trainer

    def save_model(self, trainer):
        """Save the fine-tuned model"""
        logger.info("üíæ Saving fine-tuned model...")
        
        if UNSLOTH_AVAILABLE:
            # Save with Unsloth (more efficient)
            self.model.save_pretrained(str(self.output_dir))
            self.tokenizer.save_pretrained(str(self.output_dir))
            
            # Also save in GGUF format for llama.cpp compatibility
            gguf_dir = self.output_dir / "gguf"
            gguf_dir.mkdir(exist_ok=True)
            
            try:
                self.model.save_pretrained_gguf(str(gguf_dir), tokenizer=self.tokenizer)
                logger.info(f"‚úÖ GGUF format saved to: {gguf_dir}")
            except Exception as e:
                logger.warning(f"Failed to save GGUF format: {e}")
        else:
            # Save with standard transformers
            trainer.save_model()
            self.tokenizer.save_pretrained(str(self.output_dir))
        
        # Save training metadata
        metadata = {
            "model_name": self.model_name,
            "training_data_path": str(self.training_data_path),
            "training_config": self.training_config,
            "lora_config": self.lora_config,
            "training_date": datetime.now().isoformat(),
            "max_seq_length": self.max_seq_length,
            "base_model_dir": str(self.base_model_dir),
            "output_dir": str(self.output_dir),
        }
        
        with open(self.output_dir / "training_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"‚úÖ Fine-tuned model saved to: {self.output_dir}")

    def test_model(self):
        """Test the fine-tuned model with sample inputs"""
        logger.info("üß™ Testing fine-tuned model...")
        
        # Sample test cases from your training data domain
        test_cases = [
            {
                "instruction": "You are an expert fighting game coach. Analyze this game state and provide tactical advice for the hero player.",
                "input": "Hero: 45% HP, 80% stamina, moving. Knight: 60% HP, 30% stamina, blocking. Distance: medium"
            },
            {
                "instruction": "You are an expert fighting game coach. Analyze this game state and provide tactical advice for the hero player.",
                "input": "Hero: 90% HP, 20% stamina, idle. Knight: 30% HP, 90% stamina, attacking. Distance: close"
            },
            {
                "instruction": "You are an expert fighting game coach. Analyze this game state and provide tactical advice for the hero player.",
                "input": "Hero: 10% HP, 95% stamina, blocking. Knight: 85% HP, 40% stamina, approaching. Distance: far"
            }
        ]
        
        # Generate responses with progress bar
        with tqdm(test_cases, desc="üéÆ Testing model", unit="test") as pbar:
            for i, test_case in enumerate(pbar, 1):
                pbar.set_description(f"üéÆ Testing case {i}/{len(test_cases)}")
                
                prompt = f"""<|user|>
{test_case['instruction']}

Game State: {test_case['input']}<|end|>
<|assistant|>
"""
                
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=150,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                # Extract just the assistant's response
                assistant_response = response.split("<|assistant|>")[-1].strip()
                
                print(f"\nüéÆ Test Case {i}:")
                print(f"Input: {test_case['input']}")
                print(f"Response: {assistant_response}")
                print("-" * 80)

    def run_training(self):
        """Run the complete training pipeline"""
        print("üéÆ Fighting Game AI Assistant Fine-tuning")
        print("=" * 50)
        print(f"üñ•Ô∏è  DEVICE USED FOR TRAINING: {DEVICE.upper()}")
        print("=" * 50)
        
        # Check dependencies
        if not (UNSLOTH_AVAILABLE or TRANSFORMERS_AVAILABLE):
            print("‚ùå Required libraries not found. Please install either:")
            print("1. Unsloth (recommended): pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'")
            print("2. Transformers + PEFT: pip install transformers peft datasets accelerate bitsandbytes trl")
            return
        
        try:
            # Load training data
            training_examples = self.load_training_data()
            
            # Load model with fallback strategy
            try:
                if UNSLOTH_AVAILABLE:
                    logger.info("üöÄ Attempting to load with Unsloth...")
                    self.load_model_unsloth()
                else:
                    logger.info("üîÑ Attempting to load with Transformers...")
                    self.load_model_transformers()
            except Exception as model_error:
                logger.error(f"‚ùå Model loading failed: {model_error}")
                if "get_usable_length" in str(model_error):
                    logger.error("üí° This appears to be a transformers version compatibility issue")
                    logger.error("üí° Try: pip install transformers==4.44.2 peft==0.12.0")
                raise
            
            # Prepare dataset
            dataset = self.prepare_dataset(training_examples)
            
            # Train model
            if UNSLOTH_AVAILABLE:
                trainer = self.train_unsloth(dataset)
            else:
                trainer = self.train_transformers(dataset)
            
            # Save model
            self.save_model(trainer)
            
            # Test model
            self.test_model()
            
            print("\nüéâ Fine-tuning completed successfully!")
            print(f"üìÅ Model cache: {self.base_model_dir}")
            print(f"üìÅ Fine-tuned model: {self.output_dir}")
            print("\nNext steps:")
            print("1. Test the model with more examples")
            print("2. Integrate into your game for real-time advice")
            print("3. Consider further fine-tuning with more data")
            
        except Exception as e:
            logger.error(f"‚ùå Training failed: {e}")
            raise


def main():
    """Main function to run fine-tuning"""
    
    # Check if training data exists
    training_data_path = Path("training_data/summary/training_dataset.jsonl")
    if not training_data_path.exists():
        print("‚ùå Training data not found!")
        print("Please run: python generate_training_data.py")
        return
    
    # Initialize trainer
    trainer = PhiGameAssistantTrainer(
        model_name="microsoft/Phi-3.5-mini-instruct",
        training_data_path=str(training_data_path),
        model_dir="model",
        output_dir="model/fine_tuned",
        max_seq_length=2048
    )
    
    # Run training
    trainer.run_training()


if __name__ == "__main__":
    main()
